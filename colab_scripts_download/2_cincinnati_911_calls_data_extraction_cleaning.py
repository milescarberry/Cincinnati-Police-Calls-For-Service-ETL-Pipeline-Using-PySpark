# -*- coding: utf-8 -*-
"""2_cincinnati_pyspark_practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13M-zY00B-j_erwmNxM0vvgk_4ZSsjClR
"""

from google.colab import drive

# Mounting Google Drive

drive.mount('/content/drive')

# Full code for your reference: We will use this block for setting up the environment in our future videos

# Install java 8

!apt-get update

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using

!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz

# Unzip file

!tar -xvzf spark-3.5.0-bin-hadoop3.tgz

# Install findspark: Adds Pyspark to sys.path at runtime

!pip install -q findspark

# Install pyspark

!pip install pyspark

!pip install pymongo

# Add environmental variables

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"

# findspark will locate spark in the system

import findspark

findspark.init()

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

sns.set_context('paper', font_scale = 1.4)

from plotly import express as exp, graph_objects as go, io as pio

pio.templates.default = 'ggplot2'

from plotly.subplots import make_subplots

import pickle

import datetime as dt

import requests

import pyspark

from time import sleep

from google.colab import userdata

from pprint import pprint

import os

import sys

import json

import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

warnings.filterwarnings("ignore", category=FutureWarning)

os.environ['PYSPARK_PYTHON'] = sys.executable

os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# Let's create a SparkSession instance

from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("cin_calls") \
    .config("spark.executer.memory", '44g') \
    .config("spark.executor.memoryOverhead", "44g") \
    .config("spark.driver.memory", '44g') \
    .config("spark.mongodb.input.uri", userdata.get("mongodb_url")) \
    .config("spark.mongodb.output.uri", userdata.get("mongodb_url")) \
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2') \
    .getOrCreate()

spark

# def get_data():


#     url = userdata.get('url')


#     start = dt.datetime.strftime(dt.datetime(2019, 1, 1, 0, 0, 0), "%Y-%m-%d").replace("'", "")


#     now = dt.datetime.strftime(dt.datetime.now(), "%Y-%m-%d").replace("'", "")


#     query = f"$where=create_time_incident>='2019-01-01T00:00:00.000' and create_time_incident<='{now}T23:59:59.000'&$limit=1000000000000"


#     # query = "$limit=5"


#     user_agent = userdata.get('user_agent')


#     headers = {


#         "User-Agent": user_agent


#      }


#     response = requests.get(url, query, headers = headers)


#     jsn = response.json()


#     # df = pd.DataFrame(jsn)


#     # df.to_csv("./data/cin_crime_data.csv", index = False)


#     return jsn



from functools import reduce

from pyspark.sql import DataFrame



def get_data():


    url = userdata.get('url')


    start = dt.datetime.strftime(dt.datetime(2019, 1, 1, 0, 0, 0), "%Y-%m-%d").replace("'", "")


    now = dt.datetime.strftime(dt.datetime.now(), "%Y-%m-%d").replace("'", "")


    rge = pd.date_range(start = start, end = now, freq = '1M')


    rge  = [dt.datetime.strftime(i, "%Y-%m-%d").replace("'", "") for i in rge]


    rge[0] = start


    rge[-1] = now


    datepairs = [[rge[i], rge[i + 1]] if i != len(rge) - 1 else [rge[i], now] for i in range(len(rge))]


    if datepairs[-2][1] == datepairs[-1][0]:


      datepairs = datepairs[:-1:1]



    for i in range(len(datepairs)):


      if i != 0:

        datepairs[i][0] = dt.datetime.strftime(

            dt.datetime.strptime(datepairs[i][0],

                                 "%Y-%m-%d"

                                 ) + dt.timedelta(days = 1),

            "%Y-%m-%d"

            ).replace("'", "")



    queries = [f"$where=create_time_incident>='{pair[0]}T00:00:00.000' and create_time_incident<='{pair[1]}T23:59:59.000'&$limit=1000000000000" for pair in datepairs]



    # query = f"$where=create_time_incident>='2019-01-01T00:00:00.000' and create_time_incident<='{now}T23:59:59.000'&$limit=1000000000000"

    # query = "$limit=5"


    # response = requests.get(url, query, headers = headers)


    # jsn = response.json()


    # # df = pd.DataFrame(jsn)


    # # df.to_csv("./data/cin_crime_data.csv", index = False)



    user_agent = userdata.get('user_agent')


    headers = {


        "User-Agent": user_agent


    }


    jsons = []


    for query in queries:


      response = requests.get(url, query, headers = headers)


      json = response.json()


      jsons.extend(json)


    return spark.createDataFrame(jsons)



    # return reduce(DataFrame.union, dfs)

"""##### In the cloud you can create multiple ChildSession instances."""

# Let's read our .csv file

# df = spark.read.csv(

#     "/content/drive/MyDrive/Projects/cin_crime_data_2019_2024/data/cin_crime_data.csv",

#     header = True,

#     inferSchema = True

# )


df = get_data()


df.count(), len(df.columns)

df.printSchema()

df.describe()

df.show(2)

df.select(['longitude_x', 'latitude_x']).describe().show()

# Show count of nan values by column

def show_nan(df, nan_criteria = 'nan'):


    ### Get count of both null and missing values in pyspark


    from pyspark.sql.functions import isnan, when, count, col


    # nan_df for non timestamp columns


    nan_df = df.select([(count(when(isnan(c) | col(c).isNull() | col(c).contains("N/A"), c)) / df.count()).alias(c) for c in [i for i in df.columns] if 'time' not in str(df.select([c])).lower()])


    # nan_df for timestamp columns (counting only null values)


    t_nan_df = df.select([(count(when(col(c).isNull(), c)) / df.count()).alias(c) for c in df.columns if 'time' in str(df.select([c])).lower()])



    # c_nan_df = df.select([(count(when(col(c).contains("N/A") | isnan(c) | col(c).isNull(), c)) / df.count()).alias(c) for c in [i for i in df.columns if 'neighborhood' in i.lower()]])



    # Convert from pyspark dataframe to pandas dataframe


    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")


    nan_df = nan_df.toPandas()


    t_nan_df = t_nan_df.toPandas()


    # c_nan_df = c_nan_df.toPandas()


    nan_df = pd.concat([nan_df, t_nan_df], axis = 1)


    nan_df = nan_df.T.reset_index()


    nan_df.columns = ['column', 'nan_%']


    if 'nan' not in str(nan_criteria).lower():


        nan_df = nan_df[nan_df['nan_%'] >= nan_criteria]



    fig = exp.bar(nan_df, x = 'nan_%', y = 'column')



    fig.update_layout(bargap = 0.32)



    fig.update_layout(height = 900)



    fig.show()



    return None

# show_nan(df)

from pyspark.sql.functions import year, month, dayofmonth, hour, minute, col


df_ = df.withColumn('create_time_incident_year', year(col('create_time_incident'))) \
                     .withColumn('create_time_incident_month', month(col('create_time_incident'))) \
                     .withColumn('create_time_incident_day', dayofmonth(col('create_time_incident'))) \
                     .withColumn('create_time_incident_hour', hour(col('create_time_incident'))) \
                     .withColumn('create_time_incident_minute', minute(col('create_time_incident'))) \
                     .withColumn('closed_time_incident_month', month(col('closed_time_incident'))) \
                     .withColumn('closed_time_incident_day', dayofmonth(col('closed_time_incident'))) \
                     .withColumn('closed_time_incident_hour', hour(col('closed_time_incident'))) \
                     .withColumn('closed_time_incident_minute', minute(col('closed_time_incident')))


df_ = df_.withColumn('arrival_time_primary_unit_year', year(col('arrival_time_primary_unit'))) \
                     .withColumn('arrival_time_primary_unit_month', month(col('arrival_time_primary_unit'))) \
                     .withColumn('arrival_time_primary_unit_day', dayofmonth(col('arrival_time_primary_unit'))) \
                     .withColumn('arrival_time_primary_unit_hour', hour(col('arrival_time_primary_unit'))) \
                     .withColumn('arrival_time_primary_unit_minute', minute(col('arrival_time_primary_unit')))


df_ = df_.withColumn('dispatch_time_primary_unit_year', year(col('dispatch_time_primary_unit'))) \
                     .withColumn('dispatch_time_primary_unit_month', month(col('dispatch_time_primary_unit'))) \
                     .withColumn('dispatch_time_primary_unit_day', dayofmonth(col('dispatch_time_primary_unit'))) \
                     .withColumn('dispatch_time_primary_unit_hour', hour(col('dispatch_time_primary_unit'))) \
                     .withColumn('dispatch_time_primary_unit_minute', minute(col('dispatch_time_primary_unit')))


df_.show(2)

df_.columns

from pyspark.sql.functions import col, lit, concat


df_ = df_.withColumn('coordinates',concat(col('longitude_x').cast("string"), lit(", "), col('latitude_x').cast("string")))

df_ = df_.withColumn(
    "create_closed_timedelta",
    (col("closed_time_incident").cast("timestamp").cast("long") - col("create_time_incident").cast("timestamp").cast("long")) /60
)



df_ = df_.withColumn(
    "dispatch_arrival_timedelta",
    (col("arrival_time_primary_unit").cast("timestamp").cast("long") - col("dispatch_time_primary_unit").cast("timestamp").cast("long")) / 60
)


df_ = df_.withColumn(

    "create_dispatch_timedelta",

    (col("dispatch_time_primary_unit").cast('timestamp').cast('long') - col("create_time_incident").cast("timestamp").cast("long")) / 60


    )



df_ = df_.withColumn(

    "create_arrival_timedelta",

    (col("arrival_time_primary_unit").cast('timestamp').cast('long') - col("create_time_incident").cast("timestamp").cast("long")) / 60


    )

df_.columns

from pyspark.sql.window import Window

from pyspark.sql.functions import dense_rank


windowSpec = Window.partitionBy("event_number").orderBy(col("create_time_incident").desc())

df_ranked = df_.withColumn("event_rank", dense_rank().over(windowSpec))

# df_ranked.show()

df_ranked = df_ranked.where((col('event_rank') == 1) & ~(col('district').isNull()))

df_ranked = df_ranked.drop('event_rank')

# show_nan(df_ranked)

"""### Exporting PySpark DataFrame"""

df_ranked.write.mode("overwrite").parquet("/content/drive/MyDrive/Projects/cin_crime_data_2019_2024/data/cin_crime_data.parquet")

